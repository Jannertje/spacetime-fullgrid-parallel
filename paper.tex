\documentclass[11pt,a4paper]{amsart}

\usepackage[]{algorithm2e}
\usepackage[export]{adjustbox}
\usepackage[margin=1.5in]{geometry}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{commath}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage{listings}
\usepackage{todonotes}
%\setlength{\parindent}{0pt}

\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem*{remark}{Remark}

\usepackage{color}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\providecommand\C{}
\renewcommand{\C}{\mathbb{C}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\K}{\mathcal{K}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\f}{\varphi}
\newcommand{\e}{\varepsilon}
\newcommand{\eps}{\varepsilon}
\renewcommand{\a}{\alpha}
\renewcommand{\b}{\beta}
\newcommand{\z}{\zeta}
\renewcommand{\O}{\Omega}
\renewcommand{\P}{\mathbb P}
\newcommand{\intRn}{\int_{\R^n}}
\newcommand{\sumi}{\sum_{i=1}^n}
\newcommand{\sumij}{\sum_{i,j=1}^n}
\renewcommand{\d}{\delta}
\newcommand{\p}{\partial}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\osc}{osc}
\DeclareMathOperator{\blockdiag}{blockdiag}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\gen}{gen}
\DeclareMathOperator{\cat}{Cat}
\DeclareMathOperator{\Cat}{Cat}
\DeclareMathOperator{\Div}{div}
\renewcommand{\div}{\Div}
\renewcommand{\l}{\lambda}
\providecommand{\spann}{\mathrm{span}}
\newcommand{\sm}{\setminus}
\newcommand{\es}{\emptyset}
\newcommand{\ol}{\overline}
\newcommand{\cl}{\overline}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\kron}{\otimes}
\newcommand{\inp}[2]{\langle #1, #2 \rangle}
\usepackage{stmaryrd}
\newcommand{\jump}[1]{\left\llbracket #1 \right\rrbracket}
\newcommand{\rhu}{\rightharpoonup}
\newcommand{\T}{\mathcal{T}}
\newcommand{\V}{\mathcal{V}}
\DeclareMathOperator*{\dof}{\#\!DoF}
\let\Vec\undefined
\DeclareMathOperator*{\Vec}{Vec}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Id}{Id}
\newcommand{\ip}[2]{ \langle #1, #2 \rangle}
\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}
\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
\providecommand\note{}
\renewcommand{\note}[1]{{\color{teal} [JW: #1]}}
\newcommand{\new}[1]{{\color{ao(english)} #1}}
\renewcommand{\L}{\mathcal L}
\newcommand{\Y}{\mathcal Y}
\newcommand{\Lis}{{\L{\text{is}}}}
\newcommand{\Lto}{{L_2(\Omega)}}
\newcommand{\bbT}{\mathbb{T}}

\begin{document}

\title{Fullgrid spacetime heat equation solve in optimal parallel complexity}
\date{\today}
\maketitle

We solve the heat equation on some domain $I \times \Omega$ with $I := [0, T]$
and $\Omega \subset \R^d$ using tensor-product trial- and test spaces. This
translates to, for given $\delta := (p_t, \T_I, p_x, \T_\Omega)$,
\[
  \begin{cases}
  X^\delta := X^\delta_t \kron X^\delta_x, \quad X^\delta_t := H^1(I) \cap \P_{p_t}^{-1}(\T_I), \quad X^\delta_x := H^1_0(\Omega) \cap \P_{p_x}^{-1}(\T_\Omega), \\
  Y^\delta := Y^\delta_t \kron X^\delta_x, \quad Y^\delta_t := L_2(I) \cap \P_{p_t}^{-1}(\T_I).
  \end{cases}
\]
We see that the spatial discretization on $X^\delta$ and $Y^\delta$ coincide.
We equip $X^\delta_t$ with basis $\Phi_t$, $Y^\delta_t$ with $\Psi_t$, $X^\delta_x$ with $\Phi_x$, and view them as column vectors.

In this form, the Andreev method reads as
\[
  {\bf S} {\bf u} = {\bf f}, \quad \text{where} \quad {\bf f} := {\bf B}^\top {\bf K}^{-1} {\bf g} + {\bf u}_0
\]
where
\[
  {\bf g} := \langle g, \Psi_t \rangle_{L_2(I)} \kron \langle g, \Phi_x \rangle_{L_2(\Omega)}, \quad {\bf u}_0 := \Phi_t(0) \kron \langle u_0, \Phi_x \rangle_{L_2(\Omega)}
\]
and
\begin{align*}
  {\bf S} := {\bf B}^\top {\bf K}^{-1} {\bf B} + {\bf \Gamma}_0, \quad
  {\bf B} &:= {\bf B}_1 + {\bf B}_2, \quad
  {\bf B}_1 := {\bf T}_t \kron {\bf M}_x,
  \quad {\bf B}_2 := {\bf N}_t \kron {\bf A}_x, \\
  {\bf K} &:= {\bf K}_t \kron {\bf K}_x, \quad {\bf K}_x \eqsim {\bf A}_x,
  \quad {\bf \Gamma}_0 := {\bf \Gamma}_{0,t} \kron {\bf M}_x
\end{align*}
where
\begin{align*}
  {\bf T}_t &:= \langle \frac{\dif}{\dif t} \Phi_t, \Psi_t \rangle_{L_2(I)}, \quad %\left[ \int_I \phi_t' \psi_t \dif t \right]_{\phi_t \in \Phi_t, \psi_t \in \Psi_t}, \quad
  {\bf N}_t := \langle \Phi_t, \Psi_t \rangle_{L_2(I)}, \quad %\left[ \int_I \phi_t \psi_t \dif t \right]_{\phi_t \in \Phi_t, \psi_t \in \Psi_t}, \\
  {\bf K}_t := \langle \Psi_t, \Psi_t \rangle_{L_2(I)}, \\ % \left[ \int_I \psi_t \tilde \psi_t \dif t \right]_{\psi_t,\tilde \psi_t \in \Psi_t}, \quad
  {\bf \Gamma}_{0,t} &:= \Phi_t(0) \Phi_t^\top(0), \quad %\left[ \phi_t(0) \tilde \phi_t(0)\right]_{\phi_t,\tilde\phi_t \in \Phi_t}, \\
  {\bf M}_x := \langle \Phi_x, \Phi_x \rangle_{L_2(\Omega)}, \quad %\left[ \int_\Omega \phi_x \tilde \phi_x \dif x \right]_{\phi_x,\tilde \phi_x \in \Phi_x}, \quad
  {\bf A}_x := \langle \nabla \Phi_x, \nabla \Phi_x \rangle_{L_2(\Omega)}. %\left[ \int_\Omega \nabla \phi_x \cdot \nabla \tilde \phi_x \dif x \right]_{\phi_x,\tilde \phi_x \in \Phi_x}.
\end{align*}
We note that ${\bf M}_t$ is not a square matrix (different trial and test bases), nor is ${\bf T}_t$, and that ${\Gamma}_{0,t}$ is very sparse. When $\Psi_t$ is an orthogonal basis (eg scaled Legendre polynomials), ${\bf T}_t$ is bidiagonal and ${\bf K}_t$  simply diagonal, so it can be inverted directly without the use of preconditioners. ${\bf K}_x^{-1}$ is simply a symmetric multigrid for the stiffness matrix.

\begin{lemma}
  When $Y^\delta_x = X^\delta_x$, and $X^\delta_t \subset Y^\delta_t$, the `Andreev method' coincides with the `new method', ie,
  \[
    {\bf S} = {\bf C}^{\top} {\bf K}^{-1} {\bf C} + {\bf A} + {\bf \Gamma}_T, \quad \text{and} \quad {\bf C}^{\top} {\bf K}^{-1} {\bf C} = {\bf A}_t \kron ({\bf M}_x {\bf K}_x^{-1} {\bf M}_x)
  \]
  where
  \begin{align*}
    {\bf C} &:= {\bf T}_t \kron {\bf M}_x, \quad {\bf A} := {\bf M}_t \kron {\bf A}_x, \\
    {\bf M}_t &:= \langle \Phi_t, \Phi_t \rangle_{L_2(I)}, \quad {\bf A}_t := \langle \frac{\dif}{\dif t} \Phi_t, \frac{\dif}{\dif t} \Phi_t \rangle_{L_2(I)}, \\
    {\bf \Gamma}_T &:= {\bf \Gamma}_{T,t} \kron {\bf M}_x, \quad {\bf \Gamma}_{T,t} := \Phi_t(T) \Phi_t^\top(T).
  \end{align*}
\end{lemma}
\begin{proof}
By virtue of $({\bf A} \kron {\bf C}) ({\bf B} \kron {\bf D}) = ({\bf A} {\bf B}) \kron ({\bf C} {\bf D})$ and symmetry of the spatial matrices, we can rewrite ${\bf S}$ as
  \begin{align*}
    {\bf S} = ({\bf T}_t^\top {\bf K}_t^{-1} {\bf T}_t) \kron ({\bf M}_x {\bf K}_x^{-1} {\bf M}_x) &+ ({\bf N}_t^\top {\bf K}_t^{-1} {\bf N}_t) \kron {\bf A}_x \\
    &+ ({\bf T}_t^\top {\bf K}_t^{-1} {\bf N}_t + {\bf N}_t^\top {\bf K}_t^{-1} {\bf T}_t) \kron {\bf M}_x + {\bf \Gamma}_0.
  \end{align*}
  Now, one can prove that in fact
  \[
    {\bf T}_t^\top {\bf K}_t^{-1} {\bf T}_t = {\bf A}_t, \quad {\bf N}_t^\top {\bf K}_t^{-1} {\bf N}_t = {\bf M}_t
  \]
  because $X_t^\delta \subset Y_t^\delta$. Moreover, ${\bf N}_t^\top {\bf K}_t^{-1} {\bf T}_t = \langle \frac{\dif}{\dif t} \Phi_t, \Phi_t \rangle_{L_2(I)}$ discretizes the time derivative, and hence by partial integration, we see that
  \[
    {\bf T}_t^\top {\bf K}_t^{-1} {\bf N}_t + {\bf N}_t^\top {\bf K}_t^{-1} {\bf T}_t + {\bf \Gamma}_{0,t} = {\bf \Gamma}_{T,t}
  \]
  which proves the result.
\end{proof}
\begin{remark}
  For this `new method form', it is immediately obvious that $({\bf S}{\bf x})({\bf x}) = \|{\bf x}^\top \Vec(\Phi_t \kron \Phi_x)\|^2_{X,Y^\delta}$. It is also faster to evaluate, containing less terms, and I think it immediately proves the inf-sup stability of the `Andreev method' for nonsymmetric spatial operators.

The right-hand side could be similarly rewritten as
  \[
    {\bf f} := {\bf B}^\top {\bf K}^{-1} {\bf g} + {\bf u}_0 = [({\bf T}_t^\top {\bf K}_t^{-1}) \kron ({\bf M}_x {\bf K}_x^{-1})] {\bf g} + [({\bf N}_t^\top {\bf K}_t^{-1}) \kron {\bf I}_x] {\bf g} + {\bf u}_0
  \]
  which could be more efficient to implement.
\end{remark}

\begin{remark}
  I think the `new method' may actually be a reformulation of \emph{implicit Euler} as a space-time problem. In any case, the method of Neumuller and Smears is \emph{very} close to the new method. See their paragraph 2. Their matrix ${\bf S}$ is almost equal to ours, the only two differences being that (1) they did not use the partial integration trick from the previous proof to replace their ${\bf K} + {\bf K}^\top$ with our ${\bf \Gamma}_T$, and (2) their discrete solution is discontinuous piecewise constant in time.
\end{remark}

\subsection*{Wavelets in time}
We solve this system using a wavelet-in-time, multigrid-in-space preconditioner. To this end, we
define the wavelet-transform matrix
\[
  {\bf W} := {\bf W}_t \kron {\bf I}_x
\]
and instead solve the (again self-adjoint)
\begin{equation}
  {\bf W}^\top {\bf S} {\bf W} {\bf w} = {\bf W}^\top {\bf f}
  \label{eqn:wit-mat}
\end{equation}
from which we recover ${\bf u} = {\bf W} {\bf w}$.

\subsubsection*{Construction of ${\bf W}_t$}
When $X^\delta_t$ is the space of continuous piecewise polynomials degree $p_t$ on a $J$-times dyadically refined interval,
we can write a sequence $V^0 \subset \cdots \subset V^J = X^\delta_t$ of spaces.
Let $\Phi^j_t$ be the singlescale basis on $V^j$. Denoting $W^j$ by the space satisfying
\[
  W^0 := V^0, \quad V^j = V^{j-1} \oplus W^j
\]
with basis $\Sigma^j = \{\sigma_\lambda : |\lambda| = j \}$ for each $W^j$, we write $\Sigma_t := \Sigma^0 \cup \cdots \cup \Sigma^J$.
Let ${\bf P}^j$ represent the matrix embedding $V^{j-1}$ in $V^j$, and ${\bf Q}^j$
the matrix embedding $W^j$ in $V^j$. Then
\[
  {\bf W}^0 := {\bf Q}^0, \quad {\bf W}^j := [{\bf P}^j {\bf W}^{j-1} | {\bf Q}^j], \quad {\bf W}_t := {\bf W}^J.
\]
\begin{remark}
  When $p_t = 1$, the nodal basis resp.~3-point wavelet basis satisfy the conditions of $\Phi^j$ resp.~$\Sigma^j$.
  In this case,
\[
  {\bf P}^3 = \begin{bmatrix}1 \\ 1\over2 & 1\over2 \\ & 1  \\ & 1\over2 & 1\over2 \\ &&1 \\ &&1\over2 & 1\over2 \\ &&&1 \\ &&& 1\over2 & 1\over2 \\ &&&&1\end{bmatrix}, \quad {\bf Q}^3 = 2^{3/2} \begin{bmatrix} -1 \\ 1 \\ -1\over2 & -1\over2 \\ &1 \\ &-1\over2 & -1\over2 \\ &&1 \\ &&-1\over2 & -1\over2 \\ &&&1 \\ &&&-1\end{bmatrix}.
\]
\end{remark}

\subsection*{The wavelet-in-time, multigrid-in-space preconditioner}
We will solve~\eqref{eqn:wit-mat} using preconditioned CG, first sequentially and then parallel in time.
For this, we need a preconditioner ${\bf P}$. We will build it as
\[
  {\bf P} := \blockdiag\left[ {\bf C}_{|\lambda|}^{-1} {\bf A}_x {\bf C}_{|\lambda|}^{-1} \right]_{\sigma_\lambda \in \Sigma_t}, \quad \text{where} \quad {\bf C}_{|\lambda|} \eqsim {\bf A}_x + 2^{|\lambda|} {\bf M}_x.
\]
The matrices ${\bf C}_{|\lambda|}^{-1}$ are constructed as black-box symmetric multigrid preconditioners. Note that ${\bf C}_{|\lambda|}$ only depends on \emph{the level} of the wavelet. To this end, we build preconditioners for levels $0, \ldots, J$ and reuse the same object for many wavelets.

\begin{remark}
  NGsolve allows for higher-order methods without much difficulty. Do we want to experiment with, say, orders 2,3,4,5? What do the wavelet transform and preconditioner then look like? How do we parallellize? Can we hope to build a method for arbitrary $p$? Is it possible that the conditioning is $p$-robust?
\end{remark}

\subsection*{The application of Kronecker product matrices}
To reduce memory usage, we do not build the Kronecker product matrices but instead use that for matrices ${\bf B}_t$, ${\bf B}_x$, and ${\bf X}$ of
appropriate size,
\[
  ({\bf B}_t \kron {\bf B}_x) \Vec({\bf X}) = \Vec({\bf B}_x ({\bf B}_t {\bf X}^\top)^\top)
\]
where $\Vec(\cdot)$ stacks all columns vertically. When ${\bf x} := \Vec({\bf X})$ is given as a vector, we simply reshape it.

\begin{remark}
  When ${\bf B}_x$ is selfadjoint, we may write
\[
  ({\bf B}_t \kron {\bf B}_x) \Vec({\bf X}) = \Vec({\bf B}_x {\bf X} {\bf B}_t)
\]
but I am not sure if this helps us.
\end{remark}

\subsection*{Solving sequentially}
Denote ${\bf b} := {\bf W}^\top {\bf f}$, and ${\bf T} := {\bf W}^\top {\bf S} {\bf W}$. See the preconditioned CG implementation below.
\begin{algorithm}
  ${\bf r}_0 := {\bf b} - {\bf T} {\bf w}_0$\tcp*{local-parallel-local matvec}
  ${\bf p}_0 := {\bf P}^{-1} {\bf r}_0$\tcp*{local matvec}
  ${\bf z}_0 := {\bf p}_0$\;
  $\gamma_0 := {\bf r}_0^\top {\bf z}_0$\tcp*{parallel inner product}
  \For{$k = 0, \ldots$}{
    ${\bf t}_k := {\bf T} {\bf p}_k$\tcp*{local-parallel-local matvec}
    $\alpha_k := \gamma_k / {\bf p}_k^\top {\bf t}_k$\tcp*{parallel inner product}
    ${\bf w}_{k+1} := {\bf w}_k + \alpha_k {\bf p}_k$\;
    ${\bf r}_{k+1} := {\bf r}_k - \alpha_k {\bf t}_k$\;
    \tcc{Deze early exit wil eigenlijk anders denk ik}
    \If{${\bf r}_{k+1}^\top {\bf r}_{k+1} < \eps^2$}{
      \Return{${\bf w}_k$}
    }
    ${\bf z}_{k+1} := {\bf P}^{-1} {\bf r}_{k+1}$\tcp*{local matvec}
    $\gamma_{k+1} := {\bf r}_{k+1}^\top {\bf z}_{k+1}$\tcp*{parallel inner product}
    $\beta_k := \gamma_{k+1}/\gamma_k$\;
    ${\bf p}_{k+1} := {\bf z}_{k+1} + \beta_k {\bf p}_k$\;
  }
  \caption{PCG.}
\end{algorithm}

\subsection*{Parallel solve}
The idea is to have $N_t * N_x$ processes, initially with $N_t$ machines each
holding $N_x$ processes but if that works nicely, we may be able to use massive
parallellism in space as well. We store the necessary
matrices on all machines. We distribute the temporal DoFs to the machines, which
is possible because the spatial and temporal matrices are not too large, and
perform spatial solves using shared-memory parallellism.  The wavelet transform
matrices ${\bf W}$ and ${\bf W}^\top$, and the preconditioner ${\bf P}^{-1}$, are
all block-diagonal so their application parallellizes easily, and no communication
between machines is necessary.

The primary objective is therefore to parallellize the application of ${\bf S}$. Still,
the temporal matrices involved are all \emph{banded} (even tridiagonal for $p_t = 1$),
because the basis is local, so we do not need to communicate local matvec results
\emph{globally}, but just to neighbouring machines. I think we want to use a \verb`MPI_Group` and a custom \verb`MPI_Comm` for this.

On the other hand, for
consistent PCG steps, we do need to broadcast the results of the inner products
globally.  This broadcast can be done by \verb`MPI_Comm.Allreduce(MPI.SUM, local_ip)` or something.

Some useful links:
\begin{itemize}
  \item \url{https://www.youtube.com/watch?v=I0Zupm6GRLs}
  \item \url{https://chen.pw/research/computing/mpi4py.html}
  \item \url{https://mpitutorial.com/tutorials/introduction-to-groups-and-communicators/}
  \item \url{https://github.com/NGSolve/netgen/commit/2290d9fe7267b1695c28ba286f88a7bb61201e6f}
\end{itemize}

Lets call a processor $P_n^m$ when it is number $1 \leq n \leq N_t$ and $1 \leq m \leq N_x$.
\begin{enumerate}
  \item First, we do $N_x = 1$ and $N_t = N$, ie no parallellism in space. I see
    two ways of handling the situation:
    \begin{enumerate}
      \item we let NGsolve distribute the temporal
      mesh automatically, and in this case, I am not 100\% sure if our makeshift
      temporal matrices work as intended. Moreover, NGSolve focuses on 2D/3D domains,
      and 1D support is not great.
    \item we do this manually, saying something
      like \verb`MPI_Group_incl([(n-1) % N, n, (n+1) % N)])`.
      This allows us to guide what is done more directly but may not be as easy to maintain.
    \end{enumerate}
\end{enumerate}




\end{document}
